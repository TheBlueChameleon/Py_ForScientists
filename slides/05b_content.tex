% =========================================================================== %

\begin{frame}[t,plain]
\titlepage
\end{frame}

% =========================================================================== %

\begin{frame}{Efficiency}
%
\begin{center}
	\includegraphics[width=.4\linewidth]{./gfx/02-xkcd-efficiency}\\
	\vspace{6pt}
	
	\scriptsize
	\emph{I need an extension for my research project because I spent all month trying to figure out whether learning Dvorak would help me type it faster.}

	\vspace{6pt}
	\url{https://xkcd.com/1445/}
\end{center}
%
\end{frame}

% =========================================================================== %

\begin{frame}{Scope for today}
%
\begin{itemize}
\item Measuring runtime in Python
	\begin{itemize}
	\item Manually -- \texttt{time.perf\_counter}
	\item Automatic -- \texttt{timeit.timeit}
	\item With full report -- \texttt{cProfile.run}
	\end{itemize}
\item The weak points of Python
	\begin{itemize}
	\item The Python Object model in memory
	\item Iterators
	\end{itemize}
\item Effect of containers on runtime
	\begin{itemize}
	\item Arrays, Red/Black-Trees and Hashmaps
	\item Strings
	\end{itemize}
\item How Numpy circumvents them
	\begin{itemize}
	\item The obvious measures ...
	\item ... and the black magic
	\end{itemize}
\item Lessons to learn from this
\end{itemize}
%
\end{frame}

% =========================================================================== %

\begin{frame}{Measuring Runtime -- Manually}
%
\texttt{time.perf\_counter()}
\begin{itemize}
\item Returns time in seconds since some (undefined) reference point as \inPy{float}
\item Store time value before and after algorithm, compute difference -- voilÃ 
\item Nanosecond accuracy
\item Variant: \texttt{time.perf\_counter\_ns()} -- returns \inPy{int}
\item Variant: \texttt{time.process\_time()} and \texttt{time.process\_time\_ns()} -- excludes sleep time
\item Best: Average over multiple runs of your function
\end{itemize}
%
\begin{warnbox}[Never use \texttt{time.time()} for profiling]
\footnotesize
This function is okay if you want to track longer durations, but lacks the accuracy of the \texttt{perf\_counter}.
\end{warnbox}
%
\end{frame}

% =========================================================================== %

\begin{frame}{Measuring Runtime -- Automatic}
%
Automatic: \texttt{timeit.timeit(command\_string)}
\begin{itemize}
\item Returns runtime of \texttt{command\_string} in seconds
\item Optional argument \texttt{number=integer}: repetitions to increase accuracy
	\begin{itemize}
	\item Default repetitions: 1,000,000
	\end{itemize}
\item Optional argument \texttt{setup}: commands that are executed only once
	\begin{itemize}
	\item \inPy{timeit.timeit('char in text', setup='text="sample string"; char="g"')}
	\end{itemize}
\item Optional argument \inPy{globals=globals()}: allow access to local variables
\item Details: \url{https://docs.python.org/3/library/timeit.html}
\end{itemize}
%
\begin{hintbox}[Do not underestimate ...]
\footnotesize
... the impact of \inPy{number = 1_000_000}! One millisecond runtime means one second test time!
\end{hintbox}
%
\end{frame}

% =========================================================================== %

\begin{frame}[fragile]{Measuring Runtime -- Copy and Paste Code}
%
\begin{codebox}[\texttt{time.perf\_counter}]
\begin{minted}[linenos, fontsize=\scriptsize]{python3}
import time

tic = time.perf_counter()
for repetition in range(N):
    do_things()
toc = time.perf_counter()

runtime = (toc - tic) / N
\end{minted}
\end{codebox}
%
\begin{codebox}[\texttt{timeit.timeit}]
\begin{minted}[linenos, fontsize=\scriptsize]{python3}
import timeit
import math

runtime = timeit.timeit('math.sqrt(9)', globals=globals())
\end{minted}
\end{codebox}
%
\end{frame}

% =========================================================================== %

\begin{frame}{Measuring Runtime -- Resolving Individual Functions}
%
\inPy{cProfile.run(command_string)}
\begin{itemize}
\item Prints table
	\begin{itemize}
	\item \texttt{ncalls}: number of calls made
	\item \texttt{tottime}: total time spent in the given function
	\item \texttt{percall}: quotient \texttt{tottime / ncalls}
	\item \texttt{cumtime}: cumulative time in this and all subfunctions (also works for recursive functions)
	\item Second \texttt{percall} column: \texttt{cumtime / ncalls}
	\item \texttt{filename:lineno(function)}: ... guess what
	\end{itemize}
\item Optional argument: \texttt{sort=columnheader} where \texttt{columnheader} is one of the above
\end{itemize}
%
\end{frame}

% =========================================================================== %

\begin{frame}[fragile]{Measuring Runtime -- Resolving Individual Functions}
%
\begin{itemize}
\item Context Manager for more convenient handling of multi line code
\begin{minted}{python}
with cProfile.Profile() as pr:
    setup_stuff()
    more_calls()
\end{minted}
\item Output results via \texttt{pr.print\_stats()}
\item Can be converted to a \texttt{pstats.Stats} object for nicer sorting options
\item See \url{https://docs.python.org/3/library/profile.html}
\end{itemize}
%
\end{frame}

% =========================================================================== %

\begin{frame}[fragile]{Measuring Runtime -- Copy and Paste Code}
%
\begin{codebox}[\texttt{cprofile.run}]
\begin{minted}[linenos, fontsize=\scriptsize]{python3}
import cProfile

cProfile.run('do_things()', sort='tottime')
\end{minted}
\end{codebox}
%
\begin{cmdbox}[Possible Output: \texttt{cprofile.run}]
\begin{minted}[fontsize=\tiny]{text}
         348221 function calls (332217 primitive calls) in 0.450 seconds

   Ordered by: internal time

    ncalls  tottime  percall  cumtime  percall filename:lineno(function)
     16005    0.119    0.000    0.119    0.000 main-profiler.py:11(decaying_oscillation_potential)
20000/4000    0.055    0.000    0.139    0.000 copy.py:128(deepcopy)
      2000    0.031    0.000    0.263    0.000 State.py:19(evolve)
      4000    0.018    0.000    0.105    0.000 copy.py:259(_reconstruct)
      8002    0.016    0.000    0.145    0.000 Misc.py:2(central_difference_quotient)
      4000    0.014    0.000    0.019    0.000 Particle.py:100(__iadd__)
     16004    0.013    0.000    0.130    0.000 Misc.py:8(inner)
      4000    0.013    0.000    0.190    0.000 Potential.py:146(get_force_at)
\end{minted}
\end{cmdbox}
%
\end{frame}


% =========================================================================== %

\begin{frame}[fragile]{The Weak Points of Python: The Object Model}
%
\begin{itemize}
\item Python: \emph{Everything is an Object}
	\begin{itemize}
	\item Object: \inPy{class} with (hidden) attributes: ref to type, ref to data, number of refs to object
	\end{itemize}
\item Upside: Allows very high flexibility. 
	\begin{itemize}
	\item E.\;g.: \inPy{list} can hold any data type, since it is a list of objects, and every object \enquote{knows} its own data type
	\end{itemize}
\item Downside: Every action takes several steps of indirection
	\begin{itemize}
	\item Example: \texttt{a + b}
	\item Find Object \texttt{a} in memory
	\item Find and follow reference to type (to read how to add something to \texttt{a})
	\item Find and follow reference to data of \texttt{a}
	\item Do the same for \texttt{b}
	\item Execute Code behind \inPy{(type(a)).__add__}
	\end{itemize}
\end{itemize}
%
\begin{hintbox}[Full Recursion]
\scriptsize
In Python, even \inPy{class}es and modules are \inPy{object}s. So \inPy{isinstance(type(object), object) == True}.
\end{hintbox}
%
\end{frame}

% =========================================================================== %

\begin{frame}{The Object Model: \inPy{list}s}
%
\begin{center}
\inPy{some_list[index] += 5}
\end{center}
%
\begin{itemize}
\item \texttt{some\_list} refers to an \inPy{object}
\item Read the type of \texttt{some\_list} to find out it's a \inPy{list}
\item Read the data pointer of \texttt{some\_list} to find it's contents
\item Exectute the \inPy{__getItem__} method of \inPy{list} (because of \texttt{some\_list[index]})
\item \texttt{index} is an \inPy{object}. Read type and data pointer, too
\item Get the stored element -- an \inPy{object}. Read type and data pointer; execute \inPy{__iadd__}
\item \inPy{5} is an object. Read type and data pointer.
\item Write the sum back to memory by calling \inPy{__setItem__}
\end{itemize}
%
\end{frame}

% =========================================================================== %

\begin{frame}[fragile]{Tangent: Unintuitive Behaviour Due to References}
%
\begin{tcbraster}[raster columns=2,
                  raster equal height,
                  nobeforeafter,
                  raster column skip=0.2cm]
%
\begin{codebox}[References.py]
\begin{minted}[linenos,fontsize=\scriptsize]{python3}
table_1 = [[1, 0]] * 2
table_2 = [[1, 0] for i in range(2)]

print(table_1)
print(table_2)
print()

table_1[0][1] = -1
table_2[0][1] = -1

print(table_1)
print(table_2)
\end{minted}
\end{codebox}
%
\begin{cmdbox}[Output: References.py]
\begin{minted}[fontsize=\scriptsize]{text}
[[1, 0], [1, 0]]
[[1, 0], [1, 0]]

[[1, -1], [1, -1]]
[[1, -1], [1, 0]]
\end{minted}
\end{cmdbox}
%
\end{tcbraster}
%
\end{frame}

% =========================================================================== %

\begin{frame}{Tangent: Explanation}
%
\begin{itemize}
\item \inPy{table_1 = [[1, 0]] * 2}
	\begin{itemize}
	\item Construct the element \inPy{[[1, 0]]} somewhere in memory \Thus location $A$
	\item Compute \texttt{[$A$]} \inPy{* 2} \enquote{=} \texttt{[$A$, $A$]}
	\item Store result of this computation in \texttt{table\_1}
	\item[\Thus] \texttt{table\_1} contains \emph{two references to the same object}! 
	\end{itemize}
\item \inPy{[[1, 0] for i in range(2)]}
	\begin{itemize}
	\item For each \texttt{i}, construct a new object \texttt{[1, 0]} \Thus locations $A, B$
	\item Put them together in a \inPy{list} \enquote{=} \texttt{[$A$, $B$]}
	\item[\Thus] \texttt{table\_2} contains \emph{references to two different objects}! 
	\end{itemize}
\end{itemize}
%
\begin{hintbox}[It's not a bug{,} it's a feature!]
This kind of behaviour avoids copying long lists and replaces the tedious step by copying a single number (the reference). Once you get used to it, you'll like it.
\end{hintbox}
%
\end{frame}

% =========================================================================== %

\begin{frame}[fragile]{The Weak Points of Python: Iterators}
%
\begin{itemize}
\item Remember: Iterator -- \enquote{Book keeping device}, stores how to find next object in loop
	\begin{itemize}
	\item Details: see Part 3 of this series
	\end{itemize}
\item Iterators are objects ...
\item ... with their own methods ...
\item ... and own attributes, which are again objects
\item[\Thus] A simple \inPy{for} loop does several \enquote{invisible} operations before we even touch our code! For every iteration!
\end{itemize}
%
\begin{hintbox}[Where this helps]
Iterators provide a uniform interface for traversing arbitrary structures, such as \inPy{dict}s (Hashmaps), \inPy{set}s (Red-Black-Trees), \inPy{list}s (Arrays) and even self-defined structures.
\end{hintbox}
%
\end{frame}

% =========================================================================== %

\begin{frame}{Data Structures: \inPy{list}s and \inPy{tuple}s aka Arrays}
%
\begin{itemize}
\item Consecutive data\footnote{At least, consecutive \emph{pointers} to the actual data, due to Python's object model}
\item Fastest random element access (\inPy{data[i]})
	\begin{itemize}
	\item Best/worst/average case: $\mathcal{O}(1)$ (constant)
	\end{itemize}
\item Slow search of elements (\inPy{element in data})
	\begin{itemize}
	\item Average and worst case: $\mathcal{O}(N)$ (linear), best case $\mathcal{O}(1)$
	\item Iterate over entire list, touch each element once
	\end{itemize}
\item Slow insertion (\inPy{data.insert(index, object)})
	\begin{itemize}
	\item Average: $\mathcal{O}(N)$ (linear)
	\item Need to shift elements after the insertion point\footnote{\inPy{data.append(element)} does not have this problem, so it's more likely to have constant runtime}
	\item Might need to move entire list if adjacent object to the right
	\end{itemize}
\item Reasonably fast sorting (\texttt{data.sort()})\footnote{Timsort Algorithm}
	\begin{itemize}
	\item Average and worst case: $\mathcal{O}(N \log N)$, best case: $\mathcal{O}(N)$
	\end{itemize}
\end{itemize}
%
\end{frame}

% =========================================================================== %

\begin{frame}{Data Structures: \inPy{set}s aka Red/Black Trees}
%
\begin{columns}[T]
\column{.6\linewidth}
\begin{itemize}
\item Precursor structure: binary search tree (BST)
	\begin{itemize}
	\item Ideas
		\begin{itemize}
		\item Each node of a tree stores a value
		\item Each node has up to two child nodes, called left and right
		\item The value of the left node is strictly less than all its (grand-(grand-(...))) parent nodes
		\item The subtrees are again BSTs
		\end{itemize}
	\item[\Thus] Trivially sorted, easy to search elements in
	\end{itemize}
\item Enhancement: Red/Black Tree
	\begin{itemize}
	\item BST may be unbalanced
	\item Solution: set of additional rules that are easy to check and keep the tree balanced
	\item Part of the rules is assigning a \enquote{colour} to each node, usually called red and black
	\end{itemize}
\end{itemize}
%
\column{.4\linewidth}
\includegraphics[width=.8\linewidth]{./gfx/05-bst}\\
{\tiny\url{https://www.programiz.com/dsa/binary-search-tree}}

\includegraphics[width=.8\linewidth]{./gfx/05-bst-balance}
{\tiny\url{https://appliedgo.net/balancedtree/}}
\end{columns}
%
\end{frame}

% =========================================================================== %

\begin{frame}{Data Structures: \inPy{set}s aka Red/Black Trees}
%
\begin{itemize}
\item No random element access (\inPy{data[i]})
	\begin{itemize}
	\item A tree has no indices
	\end{itemize}
\item Fast search of elements (\inPy{element in data})
	\begin{itemize}
	\item Average case: $\mathcal{O}(\log N)$
	\end{itemize}
\item Fast insertion (\inPy{data.add(element)} or \inPy{data.union(otherset)})
	\begin{itemize}
	\item Average case: $\mathcal{O}(\log N)$
	\end{itemize}
\item Trivially Sorted
	\begin{itemize}
	\item \enquote{$\mathcal{O}(1)$}
	\end{itemize}
\end{itemize}
%
\begin{hintbox}[Hidden Cost]
Noteworthy overhead compared to list due to more pointers
\end{hintbox}
%
\end{frame}

% =========================================================================== %

\begin{frame}{Data Structures: \inPy{dict}s aka Hashmaps}
%
\begin{itemize}
\item Hashing
	\begin{itemize}
	\item Function f: object \thus integer
	\item High entropy: small changes to the hashable give a very different result
	\item Low collision rate: unlikely to find two different hashables that give the same hash
	\item Python: function \inPy{hash(data)} calls \inPy{data.__hash__()}, which returns its hash
	\item See the \texttt{hashlib} (\url{https://docs.python.org/3/library/hashlib.html}) for ready made implementations
	\end{itemize}
\item Hashmaps
	\begin{itemize}
	\item Start with a list of given initial capacity $c$
	\item For each key, compute an array index $i = \text{hash}(\text{key}) \mod c$
	\item Store tuple of key and value at this index
	\item Collision? Store all results with same hash as linked list (Lingu: \emph{bucktes})
	\item Grow beyond initial capacity? Double capacity, move around elements
	\end{itemize}
\end{itemize}
%
\end{frame}

% =========================================================================== %

\begin{frame}{Data Structures: \inPy{dict}s aka Hashmaps}
%
\begin{itemize}
\item Very fast Random element access (\inPy{data[key]})
	\begin{itemize}
	\item Average case: $\mathcal{O}(1)$
	\end{itemize}
\item Very fast search of elements (\inPy{key in data})
	\begin{itemize}
	\item Average case: $\mathcal{O}(1)$
	\item Just compute hash/index, check if key matches any of those stored in the bucket
	\end{itemize}
\item Very fast insertion (\inPy{data[new_key] = new_value})
	\begin{itemize}
	\item Average case: $\mathcal{O}(1)$
	\end{itemize}
\item Reasonably fast sorting 
	\begin{itemize}
	\item $\mathcal{O}(N \log N)$
	\item Literally sorting a list
	\end{itemize}
\end{itemize}
%
\begin{hintbox}[Runtime Cost and Optimization]
\footnotesize
Runtime depends mostly on how quick the hash can be computed.\\
See this video for some ideas how to optimize the hell out of hash functions (C++):\\
\url{https://youtu.be/DMQ_HcNSOAI}
\end{hintbox}
%
\end{frame}

% =========================================================================== %

\begin{frame}{Strings}
%
\begin{itemize}
\item On C level: array of \mintinline{c}{char}s
	\begin{itemize}
	\item Operations may require extra time for allocating memory
	\item Worst case: insufficient contiguous memory may force copying the string
	\end{itemize}
\item On Python level: \inPy{str}ings are immutable
	\begin{itemize}
	\item \emph{Any} change to the string forces a copy
	\item This hits hard when you try to concatenate several strings
	\end{itemize}
\item How to properly concatenate long strings\footnote{%
This optimization is certainly worth the while if you have very many strings or put them together often. When joining only two strings, \inPy{s1 + s2} is \emph{always} faster
}
	\begin{itemize}
	\item Build a list of the partial strings
	\item Get their sizes and sum up
	\item Allocate space for the total string once
	\item Copy the list elements into the target memory
	\item[\Thus] This is was \inPy{str.join} does!
	\end{itemize}
\end{itemize}
%
\end{frame}

% =========================================================================== %

\begin{frame}[fragile]
%
\begin{codebox}[Timing String Concatenation]
\begin{minted}[linenos, fontsize=\scriptsize]{python3}
def use_join(data):
    return "".join(data)

def use_for(data):
    result = ""
    for elm in data:
        result += elm
    return result

def concatenator(acc, elm):
    return acc + elm

def use_functools(data):   # note: sum(data, "") no longer works in Python
    return functools.reduce(concatenator, data, "")

data = [f"String {i}|" for i in range(50)]

print(timeit.timeit("use_join(data)"     , globals=globals()))  # 0.2951953629999480s
print(timeit.timeit("use_for(data)",       globals=globals()))  # 2.0095849460012687s
print(timeit.timeit("use_functools(data)", globals=globals()))  # 3.1528632110021135s
\end{minted}
\end{codebox}
%
\end{frame}

% =========================================================================== %

\begin{frame}[fragile]{How Numpy Circumvents Python's Weak Points}
%
\begin{itemize}
\item Numpy is written in C
	\begin{itemize}
	\item Less flexible, less overhead
	\item Numpy-Arrays are of one single type
	\item Iteration using integer indices rather than complex iterators
	\end{itemize}
\item Numpy runs directly on the processor, not in the Python RTE
	\begin{itemize}
	\item RTE: runtime environment
	\item Python is compiled to bytecode\footnote{%
			See the module \texttt{dis} if you want to fool around on the lowest level
		}; essentially code for \enquote{Python OS}
	\item Python interpreter translates bytecode (pseudo machine language) into machine language
	\item Numpy bypasses this, \enquote{speaks directly to the processor} 
	\end{itemize}
\item Numpy uses black magic
	\begin{itemize}
	\item SIMD and other hardware level optimizations
	\item Bonus slides at the end if you're interested
	\end{itemize}
\item[\Thus] Proper use of numpy should give you about $10\times$ speedup.
\end{itemize}
%
\end{frame}

% =========================================================================== %

\begin{frame}{Some Guidelines: Pure Python Level}
%
\begin{itemize}
\item Before optimizing, run a profiler on your code!
\item Avoid repeated evaluation of the same expression
	\begin{itemize}
	\item Compute constants before loops
	\item Create lookup tables for often-needed expressions
	\end{itemize}
\item Use apt data containers
	\begin{itemize}
	\item Frequent indexed read, rare inserts \Thus \inPy{list}s
	\item Frequent indexed read, frequent inserts \Thus \inPy{dict}s
	\item Frequent check: object in container? \Thus \inPy{set}s
	\item Small datasets \Thus \inPy{list}s
	\end{itemize}
\item Use builtin functions/methods
	\begin{itemize}
	\item for \inPy{list}s: \texttt{count}, \texttt{reverse}, \texttt{sort}
	\item for \inPy{dict}s: same methods via \texttt{keys}, \texttt{values}, \texttt{items}; \texttt{fromkeys(keys, values)}
	\item for \inPy{set}s: \texttt{difference}, \texttt{intersection}
	\item for all iterables: \inPy{sum}, \inPy{min}, \inPy{max}, \inPy{reversed}
	\end{itemize}
\end{itemize}
%
\end{frame}

% =========================================================================== %

\begin{frame}{Some Guidelines: Numpy}
%
\begin{itemize}
\item Pre-allocate rather than append/insert:\\
	Prefer \inPy{data = np.zeros(size)} or \inPy{data = np.ones(size) * initial_value} plus update over inserting 
\item Avoid frequent conversions -- start with one (numpy-) datatype and stick to it
\item Avoid copies -- use views
	\begin{itemize}
	\item \inPy{data = np.random.random(size)}
	\item \inPy{view = data[3:9:2]}
	\item No copy -- \texttt{view} describes a sub-array of \texttt{data}
	\item Changes to \texttt{view} affect \texttt{data}
	\end{itemize}
\item Use numpy-builtins rather than Python builtins or own implementations
	\begin{itemize}
	\item \texttt{sum}, \texttt{product}, \texttt{min}, \texttt{max}, \texttt{argmin}, \texttt{argmax}
	\item \texttt{sin}, \texttt{cos}, \texttt{tan}, \texttt{exp}, \texttt{exp2}, \texttt{log}, ...
	\item \texttt{dot}, \texttt{@} (aka \inPy{__matmul__}), \texttt{inner}, \texttt{outer}
	\item \texttt{around}, \texttt{angle}, \texttt{take}, ...
	\end{itemize}
\end{itemize}
%
\end{frame}

% =========================================================================== %
%
\begin{frame}[fragile]{Numpy -- Copy and Paste Patterns}
%
\begin{codebox}[Pre-Evaluate Functions]
\begin{minted}[linenos, fontsize=\scriptsize]{python3}
import numpy as np

one_dimensional = np.arange(start, stop, stride)
data = np.sin(one_dimensional)

X, Y, Z, ... = np.meshgrid(values_x, values_y, values_z, ...)
data = np.sin(X*X + Y*Y) * Z
\end{minted}
\end{codebox}
%
\begin{hintbox}[Non-Numpy Functions]
\small
This usually also works with non-numpy functions \texttt{f}.
If the Python-function \texttt{f} contains statements like \inPy{if}, you can pre-compile it with \texttt{np.vectorize}.

\vspace{6pt}
See:\\
\url{https://numpy.org/doc/stable/reference/generated/numpy.vectorize.html}
\end{hintbox}
%
\end{frame}

% =========================================================================== %


\begin{frame}[fragile]{Numpy -- Copy and Paste Patterns}
%
\begin{codebox}[Conditional Operations]
\begin{minted}[linenos, fontsize=\scriptsize]{python3}
import numpy as np

data = np.random.random((5, 5)) - 0.5
mask = data > 0
sum_of_positive_values = data[mask].sum()
\end{minted}
\end{codebox}
%
\begin{codebox}[Reductions and Partial Reductions]
\begin{minted}[linenos, firstnumber=last, fontsize=\scriptsize]{python3}
total = data.sum()
sum_of_rows    = data.sum(axis=0)    # sum_i data[i, :]
sum_of_columns = data.sum(axis=1)    # sum_i data[:, i]
\end{minted}
\end{codebox}
%
\end{frame}

% =========================================================================== %

\begin{frame}[fragile]{Numpy -- Copy and Paste Patterns}
%
\begin{codebox}[Map to nearest known value]
\begin{minted}[linenos, fontsize=\scriptsize]{python3}
import numpy as np

angles = np.arange(0, 360, 0.25)
lookup_sines = np.sin(angles)

intermediate_result = get_angle()
closest_angle_idx = np.abs(angles - intermediate_result).argmin()

print(f"sin({intermediate_result:6.2f}) ~= {lookup_sines[closest_angle_idx]:4.2f}")
\end{minted}
\end{codebox}
%
\end{frame}

% =========================================================================== %

\begin{frame}[fragile]{Example Runtime Measurements}
%
\begin{columns}[T]
\column{.43\linewidth}
\begin{codebox}[Computing Sums]
\begin{minted}[linenos, fontsize=\scriptsize]{python3}
def sum_naive(data_sum):
    result = 0
    for num in data_sum:
        result += num
    return result

def sum_builtin(data_sum):
    return sum(i for i in data_sum)

def sum_numpy(data_sum):
    return np.array(data_sum).sum()

def sum_numpy_no_allocate(
        data_sum_numpy):
    return data_sum_numpy.sum()
\end{minted}
\end{codebox}
%
\column{.5\linewidth}
\begin{center}
\small
\newcolumntype{O}{>{\centering \arraybackslash}m{.34\linewidth}}
\newcolumntype{E}{>{\raggedleft\arraybackslash}m{.24\textwidth}}
\rowcolors{1}{tabhighlight}{white}
\begin{tabularx}
	{\linewidth}
	{OEE}
	\toprule[1.5pt]
	\textbf{Approach}        & {\textbf{Time}~~~~} & \textbf{Relative} \tabcrlf
	
    \texttt{sum\_naive}      &    23.703 ms & 100.00\% \\
    \texttt{sum\_builtin}    &    21.128 ms &  89.14\% \\
    \texttt{sum\_numpy}      &    24.975 ms & 105.37\% \\
    \texttt{no\_allocate}    &     0.449 ms &   1.89\% \\
    \texttt{sum\_expression} &     0.001 ms & $^{1}/_{20000}\times$ \\
	
	\bottomrule[1.5pt]
\end{tabularx}

\vspace{6pt}
\emph{\small For $N = 1\,000\,000$}
\end{center}
%
\begin{codebox}[Computing Sums]
\begin{minted}[linenos, firstnumber=last, fontsize=\scriptsize]{python3}
def sum_expression(N):
    return N * (N - 1) // 2
\end{minted}
\end{codebox}
\end{columns}
%
\end{frame}

% =========================================================================== %

\begin{frame}[fragile]{Example Runtime Measurements}
%
\vspace{-6pt}
\begin{tcbraster}[raster columns=2,
                  raster equal height,
                  nobeforeafter,
                  raster column skip=0.2cm]
%
\begin{codebox}[Building Lists]
\begin{minted}[linenos, fontsize=\scriptsize]{python3}
def list_append(N_list):
    result = []
    for i in range(N_list) :
        result.append(i)
    return result

def list_comprehension(N_list):
    return [i for i in range(N_list)]
\end{minted}
\end{codebox}
%
\begin{codebox}[Building Lists]
\begin{minted}[linenos, firstnumber=last, fontsize=\scriptsize]{python3}
def list_numpy_arange(N_list):
    return np.arange(N_list)

def list_numpy_append(N_list):
    result = np.array([])
    for i in range(N_list):
        result = np.append(result, i)
    return result
\end{minted}
\end{codebox}
%
\end{tcbraster}
%
\begin{center}
\small
\newcolumntype{O}{>{\centering \arraybackslash}m{.41\linewidth}}
\newcolumntype{E}{>{\raggedleft\arraybackslash}m{.25\textwidth}}
\rowcolors{1}{tabhighlight}{white}
\begin{tabularx}
	{\linewidth}
	{OEE}
	\toprule[1.5pt]
	\textbf{Approach}            & {\textbf{Time}~~~~} & \textbf{Relative} \tabcrlf
	
    \texttt{list\_append}        &     3.277 ms & 100.00\% \\
    \texttt{list\_comprehension} &     2.214 ms &  67.56\% \\
    \texttt{list\_numpy\_arange} &     0.040 ms &   1.22\% \\
    \texttt{list\_numpy\_append} &  1431.615 ms & $436.87\times$ \\
	
	\bottomrule[1.5pt]
\end{tabularx}
\emph{\small For N = 10,000}
\end{center}
%
\end{frame}

% =========================================================================== %

\begin{frame}{The \emph{Real} Lesson to Learn}
%
\begin{columns}
\column{.6\linewidth}
\begin{itemize}
\item Write correctly working code first
\item Profile it
\item Only then, begin to optimize
\end{itemize}

\vspace{6pt}
\begin{hintbox}[Donald Knuth says]
\emph{We should forget about small efficiencies, say 97\% of the time: premature micro optimizations are the root of all evil.}
\end{hintbox}
%
\column{.3\linewidth}
\begin{center}
\includegraphics[width=.8\linewidth]{./gfx/02-xkcd-premature-optimization}
\scriptsize
\url{https://xkcd.com/1691/}
\end{center}
\end{columns}
%
\end{frame}

% =========================================================================== %

\begin{frame}
%
\begin{center}
\Huge
Bonus Slides
\end{center}
%
\end{frame}

% =========================================================================== %

\begin{frame}{Computer Architecture}
%
\begin{itemize}
\item Processor: \emph{very limited} capabilites
	\begin{itemize}
	\item Registers: extremely fast memory cells; usually 8 byte (64 bit)
	\item Elementary operations act on these registers
	\item Elementary operations: add, multiply, store in \enquote{regular memory}, ...
	\item Some registers for specialized use only (flags register, instruction pointer, data index, ...)
	\item Not even floating point operations!
	\end{itemize}
\item FPU (floating point unit)
	\begin{itemize}
	\item Extra chip
	\item Usually available since 32bit era
	\item Embedded devices still might not have them
	\end{itemize}
\item Caches
	\begin{itemize}
	\item Fast but small memory devices (some kB to MB)
	\item Used for often needed data
	\item[\Thus] Potential for optimization: keep data close together
	\end{itemize}
\end{itemize}
%
\end{frame}

% =========================================================================== %

\begin{frame}{SIMD -- Single Instruction Multiple Data}
%
\begin{itemize}
\item Do the same instruction (\zB addition) with \emph{multiple numbers} in parallel
\item Requires specialized hardware ...
	\begin{itemize}
	\item 1997: Intel Pentium with MMX (MultiMedia eXtensions)
	\item 1999: SSE (Streaming SIMD Extensions); Also SSE2, SSE3, SSE4
	\item 2008: AVX (Advanced Vector Extensions)
	\item All of them: specialized registers with up to 512 bits (64 byte)
	\item Set of instructions such as vector addition\\
		See \url{https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html}
	\end{itemize}
\item ... and software
	\begin{itemize}
	\item Few codes trivially parallelizable
	\item Example array summation
		\begin{itemize}
		\item Partition array into N sublists
		\item Compute N sub-sums in parallel
		\item Sum up N numbers without parallelization
		\end{itemize}
	\end{itemize}
\end{itemize}
%
\end{frame}

% =========================================================================== %

\begin{frame}{Test Case: Sum Up Integers}
%
\begin{columns}[T]
\column{.42\linewidth}
\begin{itemize}
\item Compute $\sum_i^N i$
\item Python, Numpy, C and C with SIMD
\item Naive approach (Python and C)
	\begin{itemize}
	\item Initialize variable \texttt{result} to \texttt{0}
	\item For each \texttt{i} between \texttt{0} and \texttt{N}
		\begin{itemize}
		\item increase \texttt{result} by \texttt{i}
		\end{itemize} 
	\end{itemize}
\item Numpy approach
	\begin{itemize}
	\item \texttt{np.arange(N).sum()}
	\end{itemize}
\end{itemize}
%
\column{.48\linewidth}
\begin{itemize}
\item SIMD approach 1
	\begin{itemize}
	\item Initialize variable \texttt{result} to \texttt{0}
	\item Prepare AVX register \texttt{accumulator}: $8\times$ \texttt{0}
	\item Prepare AVX register \texttt{summand}
	\item For each \texttt{i} between \texttt{0} and \texttt{N} (stride \texttt{8})
		\begin{itemize}
		\item Set \texttt{summand[j] = i + j} for each j between \texttt{0} and \texttt{7}
		\item Add \texttt{summand} to \texttt{accumulator} parallelly
		\end{itemize}
	\item sum up \texttt{accumulator[j]} for each \texttt{j} between \texttt{0} and \texttt{7} and store in \texttt{result}
	\end{itemize}
\end{itemize}
\end{columns}
%
\end{frame}

% =========================================================================== %

\begin{frame}{Test Case: Sum Up Integers}
%
\begin{itemize}
\item SIMD approach 2
	\begin{itemize}
	\item Initialize variable \texttt{result} to \texttt{0}
	\item Prepare AVX register \texttt{accumulator}: $8\times$ \texttt{0}
	\item Prepare AVX register \texttt{summand}: \texttt{0} .. \texttt{7}
	\item Prepare AVX register \texttt{increment}: \texttt{8} .. \texttt{8}
	\item For each i between 0 and N (stride 8)
		\begin{itemize}
		\item Add \texttt{summand} to \texttt{accumulator} parallelly
		\item Add \texttt{increment} to \texttt{summand} parallelly
		\end{itemize}
	\item sum up \texttt{accumulator[j]} for each \texttt{j} between \texttt{0} and \texttt{7} and store in \texttt{result}
	\end{itemize}
\end{itemize}
%
\begin{hintbox}[Codes Online]
See our GRIPS page for the Python and C codes.
You will find them in \todo{specify location}
\end{hintbox}
%
\end{frame}

% =========================================================================== %

\begin{frame}{Test Case: Sum Up Integers}
%
\begin{center}
\newcolumntype{O}{>{\centering \arraybackslash}m{.25\linewidth}}
\newcolumntype{E}{>{\raggedleft\arraybackslash}m{.15\textwidth}}
\rowcolors{1}{tabhighlight}{white}
\begin{tabularx}
	{.64\linewidth}
	{OEE}
	\toprule[1.5pt]
	\textbf{Approach} & {\textbf{Time}~~~~} & \textbf{Relative} \tabcrlf
	
	Python    & 23.703 ms & 5279.06\% \\
	Numpy     &  0.449 ms &  100.00\% \\
	C (naive) &  5.066 ms & 1128.28\% \\
	C SIMD 1  &  1.589 ms &  353.90\% \\
	C SIMD 2  &  0.472 ms &  105.12\% \\
	
	\bottomrule[1.5pt]
\end{tabularx}

\vspace{6pt}
\emph{\small For $N = 1\,000\,000$}
\end{center}
%
\begin{hintbox}[Apples and Oranges]
\scriptsize
My C code is not based on a preexisting array, but rather creates the values \enquote{on the fly}. The effective runtime cost of this should not do much in the case \emph{C SIMD 2} (which is reflected in the almost equal execution time), but all of this is a little comparing apples to oranges.
\end{hintbox}
%
\end{frame}